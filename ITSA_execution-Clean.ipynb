{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1d721c5",
   "metadata": {},
   "source": [
    "### Important Definitions\n",
    "**ID** - name of folder containing single patients dcm images <br>\n",
    "**Slice** - slice number <br>\n",
    "**ITSA** - interative threshold-seeking algorithm<br>\n",
    "**ROI mask** - individual thigh after removing the subcutaneous fat and femur. Contains only muscle and inter/intra muscular fat<br>\n",
    "**F1 fat segements** - segments of fat detected in the ROI mask after 1st round of ITSA (brightest fat segments)<br>\n",
    "**F2 fat segements** - segments of fat detected in ROI mask after 2nd round of ITSA <br>\n",
    "**ROI_MuscFatAreaPix** - number of pixels in ROI mask\t<br>\n",
    "**ROI_MuscFatAreaMM** - area of the ROI mask [mm^2]<br>\n",
    "**ROI_MuscFatVolMM** - volume if the ROI mask [mm^3]\t<br>\n",
    "**F1_AreaPix** - number of pixels that correspond to the F1 fat segments in the ROI mask<br>\n",
    "**F1_AreaMM** - area of the F1 fat segments in the ROI mask [mm^2]<br>\n",
    "**F1_MuscAreaPix** - number of pixel remaining in ROI mask after removing F1 fat segments<br>\n",
    "**F1_MuscAreaMM** - area of remaining ROI mask after removing F1 fat segments [mm^2]<br>\n",
    "**F1_Perc** - percentage of F1 fat segments relative to entire ROI mask<br>\n",
    "**F1_VolMM** - \tvolume of the F1 fat segments in the ROI mask [mm^3]<br>\n",
    "**F1_MuscVolMM** - volume of remaining ROI mask after removing F1 fat segments [mm^3]<br>\n",
    "**F2_AreaPix** - number of pixels that correspond to the F2 fat segments in the ROI mask<br>\n",
    "**F2_AreaMM** - area of the F2 fat segments in the ROI mask [mm^2]<br>\n",
    "**Musc_noF2_AreaPix** - number of pixel remaining in ROI mask after removing F1 and F2 fat segments<br>\t\n",
    "**musc_no_F2_AreaMM** - area of remaining ROI mask after removing F1 and F2 fat segments [mm^2]<br>\n",
    "**F1_Intensity** - mean value of F1 fat segments <br>\t\n",
    "**F2_Intensity** - mean value of F2 fat segments\t<br>\n",
    "**F2_cfactor** - correction of partial volume in pixels, percentage of the volume occupied by IMF within a partial volumed voxel (F2_Intensity/F1_Intensity)<br>\n",
    "**F2_Perc_NOTc** - percentage of F2 fat segments relative to entire ROI mask without correcting for partial volume<br>\n",
    "**F2_VolMM_NOTc** - volume of the F2 fat segments in the ROI mask without correcting for partial volume [mm^3]<br>\n",
    "**BF_FatVol_NOTc** - sum of F1 and F2 fat segments volume without correcting for partial volume[mm^3]<br>\n",
    "**F2_VolMM_c** - volume of the F2 fat segments in the ROI mask after correcting for partial volume[mm^3]<br>\n",
    "**F2_MuscVolMMc** - volume of remaining ROI mask after removing F1 and F2 fat segments [mm^3]<br>\n",
    "after correcting for partial volume<br>\n",
    "**BF_FatVol_c** - volume from combining F1 and F2 fat segments after correcting for partial volume [mm^3]<br>\n",
    "**F2_Perc_c** - percentage of F2 fat segments relative to entire ROI mask after correcting for partial volume<br>\n",
    "**BF_Perc_c** - sum of F2_Perc and F1_Prec after correcting for partial volume<br>\n",
    "**BF_AreaPix_NOTc** - number of pixels from combining F1 and F2 fat segment masks without correcting for partial volume<br>\n",
    "**BF_AreaMM^2_NOTc** - area from combining F1 and F2 fat segment masks without correcting for partial volume [mm^2]<br>\n",
    "**Musc_noBF_AreaPix_NOTc** - number of pixels in ROI mask after removing F1 and F2 fat segments without correcting for partial volume<br>\t\n",
    "**musc_no_BF_AreaMM_NOTc** - area of ROI mask after removing F1 and F2 fat segments without correcting for partial volume [mm^2]<br>\n",
    "**BF_Perc_NOTc** - percentage of F1 and F2 fat segments relative to entire ROI mask without correcting for partial volume<br>\n",
    "**BF_VolMM_NOTc** - volume from combining F1 and F2 fat segments without correcting for partial volume [mm^3]<br>\n",
    "**Musc_noBF_VolMM_NOTc** -volume of ROI mask after removing F1 and F2 fat segments without correcting for partial volume [mm^3]<br>\n",
    "**TOTAL_FatVol_c** - sum of all fat volumes from a patient (across 15 slices) after correcting for partial volume[mm^3]<br>\n",
    "**TOTAL_MuscVol_c** - sum of all muscle volumes from a patient (across 15 slices) after correcting for partial volume[mm^3]<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c85cb10",
   "metadata": {},
   "source": [
    "### Image Processing Variables\n",
    "#### Inhomogeneity Correction\n",
    "Requires sharpening the image first the running inhomogeneity correction followed by a series of blurring, thresholding and denoising functions. The thighs are then isolated and their location noted. Another inhomogeneity correction occurs before the background is removed. The final image is the 2 thighs on a black background.\n",
    "#### Within Snake\n",
    "**alpha** - snakes energy, tendency to move away from original initiator (higher alpha=more freedom to move)-small here given that initiated snake is close to fascia <br>\n",
    "**beta** - smoothness (higher beta = smoother) <br>\n",
    "**w_line** -  attraction the dark/white (- means attracted to dark pixels, + to white pixels) <br>\n",
    "**w_edge** - attraction to edges (higher=more attraction)  <br>\n",
    "#### Island Removal\n",
    "minsize = 8 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61994cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find *** to locate where changes need to be made\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbca3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run definitions file\n",
    "%run ITSA_definitions-Clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14464393",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#bring new imports\n",
    "import shutil\n",
    "import pydicom\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Set image directory and output directory\n",
    "patpath = r\"D:\\thighimages\\\"  #***change to directory with folders containing images to process\n",
    "\n",
    "#determine total number of patients\n",
    "totpat = len([d for d in os.listdir(patpath) if os.path.isdir(os.path.join(patpath, d))])\n",
    "\n",
    "#opens first patient file to determine size\n",
    "patdir = os.path.join(patpath, \"9035449\") #***change to first folder in directory that contains images to process\n",
    "patdcm = sitk.ImageSeriesReader_GetGDCMSeriesFileNames(patdir)\n",
    "patsitk=sitk.ReadImage(patdcm)\n",
    "read_data=sitk.GetArrayFromImage(patsitk) \n",
    "\n",
    "#create empty 4D array with size from number of patients and first patient file\n",
    "shape = (totpat, read_data.shape[0], read_data.shape[1], read_data.shape[2]) \n",
    "pat4d = np.empty(shape, dtype='uint8') \n",
    "\n",
    "#start indexing\n",
    "ID_count = 0\n",
    "i = 0\n",
    "for folder in os.listdir(patpath):\n",
    "    print(folder)\n",
    "    curr_folder_path = os.path.join(patpath, folder)\n",
    "    for item in os.listdir(curr_folder_path):\n",
    "        file_full_path = os.path.join(curr_folder_path, item)\n",
    "        if item.lower().endswith((\".nrrd\", \".ds_store\")): # skip outstanding files\n",
    "            continue\n",
    "        elif os.path.isdir(file_full_path):\n",
    "            if not os.path.exists(folder_full_path):\n",
    "                os.mkdir(folder_full_path)\n",
    "            shutil.copytree(file_full_path, folder_full_path, dirs_exist_ok=True)\n",
    "        else:\n",
    "            ds = pydicom.dcmread(file_full_path)\n",
    "            read_array = ds.pixel_array\n",
    "            m = sharpen1(read_data=file_full_path, filename=item, mode=\"e_s\")          \n",
    "            m = brightness_correction(read_data=m[0], filename=m[1], mode=\"CLAHE\", strength=6.2)\n",
    "            m = blur(read_data=m[0], filename=m[1], mode=\"m_b\", strength=5)\n",
    "            m = blur(read_data=m[0], filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1], mode=\"m_b\",strength=7)\n",
    "            m = thresholding(read_data=m[0], strength=55, filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1], mode=\"m_b\", strength=5)\n",
    "            m = denoise(read_data=m[0], filename=m[1])\n",
    "            m = blur(read_data=m[0], filename=m[1], mode=\"m_b\", strength=11)\n",
    "            m = fill_contours(read_data=m[0], filename=m[1], crop=True, dilation_iterations=6)\n",
    "            m = blur(read_data=m[0], filename=m[1], mode=\"m_b\", strength=13)\n",
    "\n",
    "            f = brightness_correction(read_data=file_full_path,ds=ds, strength=2.3, filename=m[1])\n",
    "\n",
    "            f = get_cleaned_image(read_data=f[0], mask=m[0], ds=ds, filename=f[1], show_images=False) \n",
    "\n",
    "            pat4d[ID_count, i, :, :] = f\n",
    "           \n",
    "            i += 1 #add to slice counting index\n",
    "            total_pixels = m[0].size\n",
    "            black_pixels = np.count_nonzero(m[0] == 0)\n",
    "            white_pixels = np.count_nonzero(m[0] == 255)\n",
    "\n",
    "            # Calculate the percentage\n",
    "            black_percentage = (black_pixels / total_pixels) * 100\n",
    "            white_percentage = (white_pixels / total_pixels) * 100\n",
    "\n",
    "    ID_count += 1 \n",
    "    i = 0 \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878fc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "#left leg array\n",
    "shape = (len(pat4d),len(pat4d[0]),len(pat4d[0,0]),len(pat4d[0,0]) )\n",
    "lthigh_arr = np.empty(shape, dtype='uint8') \n",
    "for ID_count in range (len(pat4d)):\n",
    "    print(ID_count)\n",
    "    read_data = pat4d[ID_count,14] #check last slice as it is likely the biggest\n",
    "    rmv_count,num_columns_to_remove = GetShift(read_data,'L')\n",
    "    for i in range (len(pat4d[0])):\n",
    "        read_data = pat4d[ID_count,i] \n",
    "        centered_image = LegSepNew(read_data,'L',rmv_count,num_columns_to_remove)\n",
    "        new_centered_image = centered_image[:,:] \n",
    "        lthigh_arr[ID_count, i, :, :] = new_centered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c98e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#right leg array\n",
    "shape = (len(pat4d),len(pat4d[0]),len(pat4d[0,0]),len(pat4d[0,0]) ) \n",
    "rthigh_arr = np.empty(shape, dtype='uint8') \n",
    "for ID_count in range (len(pat4d)):\n",
    "    print(ID_count)\n",
    "    read_data = pat4d[ID_count,14] #check last slice as it is likely the biggest\n",
    "    rmv_count,num_columns_to_remove = GetShift(read_data,'R')\n",
    "    for i in range (len(pat4d[0])):\n",
    "        read_data = pat4d[ID_count,i] \n",
    "        centered_image = LegSepNew(read_data,'R',rmv_count,num_columns_to_remove)\n",
    "        new_centered_image = centered_image[:,:] \n",
    "        rthigh_arr[ID_count, i, :, :] = new_centered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89883726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check all images for cutoff errors (looking at 10 patients at a time)\n",
    "j=15\n",
    "for i in range(10):\n",
    "    i = i + 0 #***change i to cycle through all images\n",
    "    stackimages(rthigh_arr[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae301da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correcting any errors in thigh separation\n",
    "def LegSepNew (image, side, rmv_count, num_columns_to_remove):\n",
    "    read_data = image\n",
    "    num_columns_to_remove = num_columns_to_remove\n",
    "    rmv_count = rmv_count\n",
    "    read_data = cv2.normalize(read_data, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    kernel = np.array([[0, 1, 0],\n",
    "                [1, 1, 1],\n",
    "                [0, 1, 0]], dtype = np.uint8)\n",
    "    erode = cv2.erode(read_data, kernel, iterations=1)\n",
    "    blur = cv2.medianBlur(erode, 3)\n",
    "    blur = cv2.bilateralFilter(blur,9,10,50)                                               #locating 2 large bright regions (legs)\n",
    "    ret, thresh = cv2.threshold(blur, 30, 255,cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_NONE)\n",
    "    largest_contours = sorted([cv2.contourArea(obj) for obj in contours], reverse=True)[:2]\n",
    "    thighs = [obj for obj in contours if cv2.contourArea(obj) in largest_contours]\n",
    "    backtorgb = cv2.cvtColor(thresh,cv2.COLOR_GRAY2RGB)\n",
    "    cv2.drawContours(backtorgb, [thighs[0]], -1, (255,0,0), 1)\n",
    "    cv2.drawContours(backtorgb, [thighs[1]], -1, (0,0,255), 1)\n",
    "    min_centroid = ()\n",
    "    max_centroid = ()\n",
    "    if side == 'L':\n",
    "            for contour in thighs:\n",
    "                # Calculate the centroid of the contour\n",
    "                M = cv2.moments(contour)\n",
    "                cX = int(M['m10'] / M['m00'])\n",
    "                cY = int(M['m01'] / M['m00'])\n",
    "                if len(min_centroid) == 0 or cX < min_centroid[0]:     #chooses centroid with smaller x value (i.e. left side)\n",
    "                    min_centroid = (cX, cY, contour)\n",
    "            min_contour = min_centroid[2]\n",
    "            end_x = min_contour[:, :, 0].max() # Find the ending x-coordinate (rightmost point) of the contour\n",
    "            #check if columns were removed in sample slice\n",
    "            end_x=251 #***change to x value that matches a slice from same patient that did not have an error\n",
    "            if rmv_count > 0:\n",
    "                centered_image = read_data[:,num_columns_to_remove:end_x]\n",
    "                start_x = min_contour[:, :, 0].min()\n",
    "                \n",
    "                if start_x < num_columns_to_remove:\n",
    "                    print(\"using backup\")\n",
    "                    centered_image = BackupLegSeparation(read_data, \"L\")\n",
    "            else:\n",
    "                centered_image = read_data[:, :end_x] #cut off at centroid\n",
    "            \n",
    "            #reform new shape\n",
    "            desired_shape = (256, 256)  # define desired shape (rows, columns)\n",
    "            np.expand_dims(centered_image,axis=2)\n",
    "            if desired_shape[1] - centered_image.shape[1] > 0: #add columns\n",
    "                num_columns_to_add = desired_shape[1] - centered_image.shape[1]\n",
    "                centered_image_f = np.pad(centered_image, ((0, 0), (0, num_columns_to_add)), mode='constant')\n",
    "            elif desired_shape[1] - centered_image.shape[1] < 0: #remove columns\n",
    "                num_columns_to_remove = centered_image.shape[1] - desired_shape[1]\n",
    "                centered_image_f = centered_image[:,:desired_shape[1]]\n",
    "                print(\"Too Big\")\n",
    "            else:\n",
    "                centered_image_f = centered_image              \n",
    "                         \n",
    "                \n",
    "    if side == 'R':\n",
    "        max_cX = None\n",
    "        for contour in thighs:\n",
    "            # Calculate the centroid of the contour\n",
    "            M = cv2.moments(contour)\n",
    "            cX = int(M['m10'] / M['m00'])\n",
    "            cY = int(M['m01'] / M['m00'])\n",
    "            if max_cX is None or cX > max_cX:\n",
    "                max_cX = cX  # Update max_cX with the current cX\n",
    "                max_centroid = (cX, cY, contour) #pick right side\n",
    "\n",
    "        max_contour = max_centroid[2]\n",
    "        start_x = max_contour[:, :, 0].min()# Find the ending x-coordinate (leftmost point) of the contour\n",
    "        desired_shape = (256, 256)  # define desired shape (rows, columns)\n",
    "        start_x=259 #***change to x value that matches a slice from same patient that did not have an error\n",
    "        #check if columns were removed in sample slice\n",
    "        if rmv_count > 0:\n",
    "            cutoff = 512 - num_columns_to_remove\n",
    "            centered_image = read_data[:,start_x:cutoff]\n",
    "            start_x = max_contour[:, :, 0].max()\n",
    "        else:\n",
    "            centered_image = read_data[:, start_x:]\n",
    "        \n",
    "        \n",
    "        if desired_shape[1] - centered_image.shape[1] > 0: #add columns\n",
    "            num_columns_to_add = desired_shape[1] - centered_image.shape[1]\n",
    "            centered_image_f = np.pad(centered_image, ((0, 0), (num_columns_to_add,0)), mode='constant')\n",
    "        elif desired_shape[1] - centered_image.shape[1] < 0: #remove columns\n",
    "            num_columns_to_remove = centered_image.shape[1] - desired_shape[1]\n",
    "            centered_image_f = centered_image[:, num_columns_to_remove:]\n",
    "        else:\n",
    "            centered_image_f = centered_image\n",
    "            \n",
    "    return centered_image_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing any errors cutoffs\n",
    "ID_count = 37 #***change to patient index that had error in thigh separation\n",
    "i = 14 #***change to slice index that had error in thigh separation\n",
    "\n",
    "#confirm correct image was selected\n",
    "plt.imshow(rthigh_arr[ID_count][i]) #***adjust lthigh_arr and rthigh_arr depending on which thigh cut offs are being fixed\n",
    "plt.show()\n",
    "\n",
    "read_data = pat4d[ID_count,14]\n",
    "rmv_count,num_columns_to_remove = GetShift(read_data,'R') #***adjust L and R depending on which thigh cut offs are being fixed\n",
    "read_data = pat4d[ID_count,i] \n",
    "centered_image = LegSepNew(read_data,'R',rmv_count,num_columns_to_remove) #***adjust L and R depending on which thigh cut offs are being fixed\n",
    "new_centered_image = centered_image[:,:] #centered image is 3D array need to just take usable parts\n",
    "rthigh_arr[ID_count, i, :, :] = new_centered_image #***adjust lthigh_arr and rthigh_arr depending on which thigh cut offs are being fixed\n",
    "\n",
    "#visualize corrected image\n",
    "plt.imshow(rthigh_arr[ID_count][i])#***adjust lthigh_arr and rthigh_arr depending on which thigh cut offs are being fixed\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beca6cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "I8c_subcfat_imp_arr=[]\n",
    "I8c_subcfat_imp_arr=rthigh_arr #***adjust lthigh_arr and rthigh_arr depending on which side is being processed\n",
    "\n",
    "I8c_roi_imp_arr=[]\n",
    "I8c_roi_imp_arr=rthigh_arr #***adjust lthigh_arr and rthigh_arr depending on which side is being processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b510a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ID_count,files in enumerate(os.listdir(patpath)):\n",
    "        print(f\"{ID_count}.{files}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6788c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create masks only\n",
    "j=15\n",
    "\n",
    "Ivol8_OG=[]\n",
    "Ivol8_L=[]\n",
    "Ivol8c_subcfat_L=[]  \n",
    "Ivol8c_roi_L=[]\n",
    "Ivol8c_c_s_L=[]\n",
    "subcfat_ring_L=[]\n",
    "musc_mask_final_L=[]\n",
    "t_L=[]\n",
    "musc_mask2_L=[] \n",
    "snake1_mask_L=[]\n",
    "bone_z_L=[]\n",
    "bonem_final_L=[]\n",
    "bonem_c_L=[]\n",
    "roi_mask_L=[]\n",
    "fatseg_mask_final_R1_L=[]\n",
    "fatseg_final_R1_L=[]\n",
    "fatseg_mask_final_R2_L=[]\n",
    "fatseg_final_R2_L=[]\n",
    "for_zcheck_S2R1_L=[]\n",
    "fatseg1_mask_final_S1_L=[]\n",
    "fatseg1_final_S1_L=[]\n",
    "fatseg2_mask_final_S1_L=[]\n",
    "fatseg2_final_S1_L=[]\n",
    "final_fat_mask_L=[]\n",
    "final_fat_L=[]\n",
    "\n",
    "\n",
    "for ID_count, files in enumerate(os.listdir(patpath)):\n",
    "    #GET IMAGES\n",
    "    image_path = os.path.join(patpath,files)\n",
    "    slice_filenames = sitk.ImageSeriesReader_GetGDCMSeriesFileNames(image_path)\n",
    "    image_OG = sitk.ReadImage(slice_filenames)\n",
    "    Ivol8_OG,j_OG,plotx=getimages(image_OG) \n",
    "    \n",
    "    #APPLY START SLICE\n",
    "    Ivol8c_subcfat=I8c_subcfat_imp_arr[ID_count]\n",
    "    Ivol8c_roi=I8c_roi_imp_arr[ID_count]\n",
    "    Ivol8=Ivol8_OG\n",
    "\n",
    "    #Extend ALL four sides by 40 pixels \n",
    "    Ivol8,pixels=enlarge_size(Ivol8,40)\n",
    "    Ivol8c_subcfat,pixels=enlarge_size(Ivol8c_subcfat,40)\n",
    "    Ivol8c_roi,pixels=enlarge_size(Ivol8c_roi,40)\n",
    "\n",
    "    Ivol8_L.append([])\n",
    "    Ivol8_L[ID_count]=Ivol8\n",
    "\n",
    "    Ivol8c_roi_L.append([])\n",
    "    Ivol8c_roi_L[ID_count]=Ivol8c_roi\n",
    "\n",
    "    Ivol8c_subcfat_L.append([])\n",
    "    Ivol8c_subcfat_L[ID_count]=Ivol8c_subcfat\n",
    "    try:\n",
    "        #MUSCLE MASK\n",
    "        subcfat=get_subcfatvol(Ivol8c_subcfat)\n",
    "        musc_fat_mask=get_musc_fat_mask(Ivol8c_subcfat)\n",
    "        subcfat_ring=get_subcfat_ring(musc_fat_mask)\n",
    "\n",
    "        curvature_flow=apply_CurvatureFilter(Ivol8c_subcfat*musc_fat_mask,Ivol8c_roi) \n",
    "        Ivol8c_c_s=enhance_sharpness(curvature_flow)\n",
    "        subcfatvol=get_subcfatvol(Ivol8c_c_s) \n",
    "\n",
    "        subcfatvol_augmented=subcfatvol+subcfat_ring \n",
    "        th, subcfatvol_augmented = cv2.threshold(subcfatvol_augmented, 0, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "        #--for calf:\n",
    "        subcfatvol_augmented = np.uint8(morphology.remove_small_holes(label(subcfatvol_augmented),area_threshold=300, connectivity=1))\n",
    "        th, subcfatvol_augmented = cv2.threshold(subcfatvol_augmented, 0, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "\n",
    "\n",
    "        t, musc_mask2=darkpieces(subcfatvol_augmented)\n",
    "\n",
    "\n",
    "        overshoot1,overshoot_removed1=remove_subcfat_overshoot(musc_mask2,Ivol8c_c_s,1)\n",
    "        overshoot2,overshoot_removed2=remove_subcfat_overshoot(overshoot_removed1,Ivol8c_c_s,2)\n",
    "        b=keep_overlaps(overshoot_removed2,1)\n",
    "        musc_mask2=b\n",
    "\n",
    "\n",
    "        mask_contours, contour_coords_L=get_contours(musc_mask2)\n",
    "        hull, hull_demonstration,hull_coords=convex_hull(contour_coords_L)\n",
    "        hullmask=floodfillall(hull) \n",
    "\n",
    "        hullmask_contour, hull_coords2=get_contours(hullmask)\n",
    "\n",
    "\n",
    "        initiator_coords=compatible_coordlist(hull_coords2) \n",
    "\n",
    "        #REMOVE VESSELS\n",
    "        vessels_removed_mask, vessels_present=remove_vessels(Ivol8c_c_s)\n",
    "        pre_vessels_mask=vessels_removed_mask-vessels_present\n",
    "        vessels_mask=get_vessels()\n",
    "\n",
    "        whites=get_whites() #generate white mask for snakes to follow \n",
    "        snake_im=get_snakeim()\n",
    "        bilateral_t=bilat_fil_snake_im(snake_im)\n",
    "        snake1_coords=primary_snake()\n",
    "        snake1_mask=snaketomask(snake1_coords)\n",
    "\n",
    "\n",
    "        snake1_mask_L.append([])\n",
    "        snake1_mask_L[ID_count]=snake1_mask\n",
    "\n",
    "\n",
    "        #BONE REMOVAL\n",
    "        musc_mask_final=snake1_mask\n",
    "        bone_prep=get_boneprep(Ivol8c_subcfat)\n",
    "        bone_prep2=find_potentialbone(bone_prep)\n",
    "\n",
    "\n",
    "        bone_z,bone_nonz=Z_connectivity_w_adj(bone_prep2)\n",
    "\n",
    "        bone_z_L.append([])\n",
    "        bone_z_L[ID_count]=bone_z\n",
    "\n",
    "\n",
    "        bonem_final=find_potentialbone2(bone_z) \n",
    "        bonem_final_L.append([])\n",
    "        bonem_final_L[ID_count]=bonem_final\n",
    "\n",
    "\n",
    "\n",
    "        cortical=corticaloutline(Ivol8c_c_s)\n",
    "        bonem_c=merge_bones(cortical,bonem_final)\n",
    "\n",
    "        bonem_c_L.append([])\n",
    "        bonem_c_L[ID_count]=bonem_c\n",
    "\n",
    "        roi_mask, roi=get_roi(snake1_mask,bonem_c,Ivol8c_subcfat)  \n",
    "\n",
    "        roi_mask_L.append([])\n",
    "        roi_mask_L[ID_count]=roi_mask\n",
    "        print(f\"{ID_count}. {files} success\")\n",
    "\n",
    "    except: #skip any errors\n",
    "        print(f\"{ID_count}. {files} encountered an error\")\n",
    "        #create placeholders\n",
    "        snake1_mask_L.append(None)\n",
    "        roi_mask_L.append(None)\n",
    "        bonem_c_L.append(None)\n",
    "        bonem_final_L.append(None)\n",
    "        bone_z_L.append(None)\n",
    "        for_zcheck_S2R1_L.append(None)\n",
    "        fatseg1_mask_final_S1_L.append(None)\n",
    "        fatseg1_final_S1_L.append(None)\n",
    "        fatseg2_mask_final_S1_L.append(None)\n",
    "        fatseg2_final_S1_L.append(None)\n",
    "        final_fat_mask_L.append(None)\n",
    "        final_fat_L.append(None)\n",
    "        fatseg_mask_final_R1_L.append(None)\n",
    "        fatseg_final_R1_L.append(None)\n",
    "        fatseg_mask_final_R2_L.append(None)\n",
    "        fatseg_final_R2_L.append(None)\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c242c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through all ROIs and classify quality\n",
    "data = pd.DataFrame(columns=['ID','slice','Button Clicked'])\n",
    "for ID_count,files in enumerate(os.listdir(patpath)):\n",
    "    if ID_count != 58: #***change to skip any patients that were not able to generate a mask (i.e. error in previous step)\n",
    "        quality_check(Ivol8c_roi_L[ID_count],roi_mask_L[ID_count],f\"{ID_count}. {files}\")\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a578dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cycle through entire dataframe\n",
    "output_dir = r\"D:\\thighimages_tags\\\" #***change to output directory for tags to be fixed using Sliceomatic\n",
    "redo = []\n",
    "for i in range(len(data)):\n",
    "    ID = str(data['ID'][i])\n",
    "    ID_count = int(ID.split('.')[0]) #get patient index\n",
    "    slic = data['slice'][i].astype(int) - 1 # get slice index\n",
    "    button = str(data['Button Clicked'][i]) # determine which button applies\n",
    "    if button == 'Good': #skip patient with good ROI\n",
    "        continue\n",
    "    if button == 'Overshoot' or button == 'Undershoot':\n",
    "        #create original dicom\n",
    "        img = sitk.GetImageFromArray(Ivol8c_roi_L[ID_count][slic]) \n",
    "        output_filename=os.path.join(output_dir, f'{ID}_Slice{slic}.dcm') \n",
    "        sitk.WriteImage(img, output_filename) \n",
    "        print(f\"{ID}.{slic} Image success\")\n",
    "        #create tags\n",
    "        stid = ID.split('.')[1]\n",
    "        volsitk = sitk.GetImageFromArray(Ivol8c_roi_L[ID_count][slic]) #sitk of original image\n",
    "        imfbonemsc = roi_mask_L[ID_count][slic]#numpy array of roi\n",
    "        slc = slic #identify the slice number but ones isnt 3d\n",
    "        writetag()\n",
    "        root, root2 = writetofolder(output_filename)\n",
    "        print(f\"{ID}.{slic} ROI success\")\n",
    "    if button == 'Incorrect Thigh Cut':\n",
    "        redo.append((ID,slic)) #add patiient file name and slice number to redo list to be redone later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ba8809",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run through ITSA with corrected tags\n",
    "path = r\"D:\\thighimages\\\" #***change to directory with folders containing images to process\n",
    "tag_path = r\"D:\\thighimages_tagscorrected\\\" #***change to directory with folders containing edited tags\n",
    "Ivol8_OG=[]\n",
    "Ivol8_L=[]\n",
    "Ivol8c_subcfat_L=[]  \n",
    "Ivol8c_roi_L=[]\n",
    "Ivol8c_c_s_L=[]\n",
    "subcfat_ring_L=[]\n",
    "musc_mask_final_L=[]\n",
    "t_L=[]\n",
    "musc_mask2_L=[]\n",
    "snake1_mask_L=[]\n",
    "bone_z_L=[]\n",
    "bonem_final_L=[]\n",
    "bonem_c_L=[]\n",
    "roi_mask_L=[]\n",
    "fatseg_mask_final_R1_L=[]\n",
    "fatseg_final_R1_L=[]\n",
    "fatseg_mask_final_R2_L=[]\n",
    "fatseg_final_R2_L=[]\n",
    "for_zcheck_S2R1_L=[]\n",
    "fatseg1_mask_final_S1_L=[]\n",
    "fatseg1_final_S1_L=[]\n",
    "fatseg2_mask_final_S1_L=[]\n",
    "fatseg2_final_S1_L=[]\n",
    "final_fat_mask_L=[]\n",
    "final_fat_L=[]\n",
    "shape = (15,336,336)\n",
    "roi_tag = np.empty(shape, dtype='uint8') \n",
    "roi = np.empty(shape, dtype='uint8')\n",
    "img_hold = np.empty(shape, dtype='uint8')\n",
    "count = 0\n",
    "slcs = []\n",
    "shape2 = (3,15,336,336)\n",
    "roi_mask_L = []\n",
    "\n",
    "\n",
    "for ID_count, files in enumerate(os.listdir(path)):\n",
    "    #GET IMAGES\n",
    "    image_path = os.path.join(path,files)\n",
    "    slice_filenames = sitk.ImageSeriesReader_GetGDCMSeriesFileNames(image_path)\n",
    "    image_OG = sitk.ReadImage(slice_filenames)\n",
    "\n",
    "\n",
    "    Ivol8_OG,j_OG,plotx=getimages(image_OG) #get all images\n",
    "    \n",
    "    #APPLY START SLICE\n",
    "    Ivol8c_subcfat=I8c_subcfat_imp_arr[ID_count]\n",
    "    Ivol8c_roi=I8c_roi_imp_arr[ID_count]\n",
    "    Ivol8=Ivol8_OG\n",
    "\n",
    "    #Extend ALL four sides by 40 pixels \n",
    "    Ivol8,pixels=enlarge_size(Ivol8,40)\n",
    "    Ivol8c_subcfat,pixels=enlarge_size(Ivol8c_subcfat,40)\n",
    "    Ivol8c_roi,pixels=enlarge_size(Ivol8c_roi,40)\n",
    "\n",
    "    Ivol8_L.append([])\n",
    "    Ivol8_L[ID_count]=Ivol8\n",
    "\n",
    "    Ivol8c_roi_L.append([])\n",
    "    Ivol8c_roi_L[ID_count]=Ivol8c_roi\n",
    "\n",
    "    Ivol8c_subcfat_L.append([])\n",
    "    Ivol8c_subcfat_L[ID_count]=Ivol8c_subcfat\n",
    "    if ID_count == 54: #***change to skip any patients that were not able to generate a mask (i.e. error in previous step)\n",
    "        print(f\"{ID_count}. {files} encountered an error\")\n",
    "        #create placeholders\n",
    "        snake1_mask_L.append(None)\n",
    "        roi_mask_L.append(None)\n",
    "        bonem_c_L.append(None)\n",
    "        bonem_final_L.append(None)\n",
    "        bone_z_L.append(None)\n",
    "        for_zcheck_S2R1_L.append(None)\n",
    "        fatseg1_mask_final_S1_L.append(None)\n",
    "        fatseg1_final_S1_L.append(None)\n",
    "        fatseg2_mask_final_S1_L.append(None)\n",
    "        fatseg2_final_S1_L.append(None)\n",
    "        final_fat_mask_L.append(None)\n",
    "        final_fat_L.append(None)\n",
    "        fatseg_mask_final_R1_L.append(None)\n",
    "        fatseg_final_R1_L.append(None)\n",
    "        fatseg_mask_final_R2_L.append(None)\n",
    "        fatseg_final_R2_L.append(None)\n",
    "\n",
    "    else:\n",
    "        #MUSCLE MASK\n",
    "        subcfat=get_subcfatvol(Ivol8c_subcfat)\n",
    "        musc_fat_mask=get_musc_fat_mask(Ivol8c_subcfat)\n",
    "        subcfat_ring=get_subcfat_ring(musc_fat_mask)\n",
    "\n",
    "        curvature_flow=apply_CurvatureFilter(Ivol8c_subcfat*musc_fat_mask,Ivol8c_roi) \n",
    "        Ivol8c_c_s=enhance_sharpness(curvature_flow)\n",
    "        subcfatvol=get_subcfatvol(Ivol8c_c_s) \n",
    "\n",
    "        subcfatvol_augmented=subcfatvol+subcfat_ring \n",
    "        th, subcfatvol_augmented = cv2.threshold(subcfatvol_augmented, 0, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "        #--for calf:\n",
    "        subcfatvol_augmented = np.uint8(morphology.remove_small_holes(label(subcfatvol_augmented),area_threshold=300, connectivity=1))\n",
    "        th, subcfatvol_augmented = cv2.threshold(subcfatvol_augmented, 0, 1, cv2.THRESH_BINARY)\n",
    "\n",
    "        t, musc_mask2=darkpieces(subcfatvol_augmented)\n",
    "\n",
    "\n",
    "        overshoot1,overshoot_removed1=remove_subcfat_overshoot(musc_mask2,Ivol8c_c_s,1)\n",
    "        overshoot2,overshoot_removed2=remove_subcfat_overshoot(overshoot_removed1,Ivol8c_c_s,2)\n",
    "        b=keep_overlaps(overshoot_removed2,1)\n",
    "        musc_mask2=b\n",
    "\n",
    "\n",
    "        mask_contours, contour_coords_L=get_contours(musc_mask2)\n",
    "        hull, hull_demonstration,hull_coords=convex_hull(contour_coords_L)\n",
    "        hullmask=floodfillall(hull) \n",
    "\n",
    "        hullmask_contour, hull_coords2=get_contours(hullmask)\n",
    "\n",
    "\n",
    "        initiator_coords=compatible_coordlist(hull_coords2) \n",
    "        \n",
    "        #REMOVE VESSELS\n",
    "        vessels_removed_mask, vessels_present=remove_vessels(Ivol8c_c_s)\n",
    "        pre_vessels_mask=vessels_removed_mask-vessels_present\n",
    "        vessels_mask=get_vessels()\n",
    "\n",
    "        whites=get_whites() #generate white mask for snakes to follow \n",
    "        snake_im=get_snakeim()\n",
    "        bilateral_t=bilat_fil_snake_im(snake_im)\n",
    "        snake1_coords=primary_snake()\n",
    "        snake1_mask=snaketomask(snake1_coords)\n",
    "\n",
    "\n",
    "        snake1_mask_L.append([])\n",
    "        snake1_mask_L[ID_count]=snake1_mask\n",
    "\n",
    "\n",
    "        #BONE REMOVAL\n",
    "        musc_mask_final=snake1_mask\n",
    "        bone_prep=get_boneprep(Ivol8c_subcfat)\n",
    "        bone_prep2=find_potentialbone(bone_prep)\n",
    "\n",
    "\n",
    "        bone_z,bone_nonz=Z_connectivity_w_adj(bone_prep2)\n",
    "\n",
    "        bone_z_L.append([])\n",
    "        bone_z_L[ID_count]=bone_z\n",
    "\n",
    "\n",
    "        bonem_final=find_potentialbone2(bone_z) \n",
    "        bonem_final_L.append([])\n",
    "        bonem_final_L[ID_count]=bonem_final\n",
    "\n",
    "\n",
    "        cortical=corticaloutline(Ivol8c_c_s)\n",
    "        bonem_c=merge_bones(cortical,bonem_final)\n",
    "\n",
    "        bonem_c_L.append([])\n",
    "        bonem_c_L[ID_count]=bonem_c\n",
    "\n",
    "        roi_mask, roi=get_roi(snake1_mask,bonem_c,Ivol8c_subcfat)  \n",
    "        roi_mask_L.append([])\n",
    "        roi_mask_L[ID_count]=roi_mask\n",
    "\n",
    "    #look for edited tags and update ROI here\n",
    "        checking_file = os.path.join(tag_path, files)\n",
    "        # Check if the file has any associated tags\n",
    "        if os.path.exists(checking_file):\n",
    "            folder = os.path.join(checking_file,\"OutputTags\")\n",
    "            tagarray, tagsln, xdim, ydim  = tagtoarray(folder) #get all tags from folder\n",
    "            for filename in os.listdir(folder):\n",
    "                # Extract slice number\n",
    "                slpos = filename.find(\".dcm.tag\")\n",
    "                if slpos < 1: #does not have .dcm.tag and therefore is regular image\n",
    "                    image = getOG(os.path.join(folder,filename))\n",
    "                    img_hold[count] = image\n",
    "                elif slpos > 1 and filename[slpos-1].isdigit():\n",
    "                    # Collect all consecutive digits preceding \".dcm.tag\"\n",
    "                    digit_str = \"\"\n",
    "                    index = slpos - 1\n",
    "\n",
    "                    while index >= 0 and filename[index].isdigit():\n",
    "                        digit_str = filename[index] + digit_str\n",
    "                        index -= 1\n",
    "                    slicenum = int(digit_str.replace(\"_\", \"\")) #gives corresponding slice number\n",
    "                    slcs.append(slicenum)\n",
    "                    count +=1 \n",
    "            #goes through all slices and assigns roi based on slice number\n",
    "            for k in range(count):\n",
    "                roi_tag[k] = tagarray[k]\n",
    "                q = slcs[k]\n",
    "                roi[q] = roi_tag[k] * img_hold[k]\n",
    "                roi_mask_L[ID_count][q]=roi_tag[k]\n",
    "            count = 0 #reset counter\n",
    "\n",
    "        #========ITSA ========#\n",
    "        #Set 1: Round 1:\n",
    "        initial_th_S1R1=initial_th(roi)\n",
    "        th_S1R1,fatseg_mask_S1R1,fatseg_S1R1=apply_ITSA_no_Z(roi,initial_th_S1R1,8) \n",
    "        #Set 1: Round 2:\n",
    "        roi_for_S1R2=subtract_fat1(fatseg_S1R1)\n",
    "        initial_th_S1R2=initial_th(roi_for_S1R2)\n",
    "        th_S1R2,fatseg_mask_S1R2,fatseg_S1R2=apply_ITSA_no_Z(roi_for_S1R2,initial_th_S1R2,8) \n",
    "    \n",
    "\n",
    "        #Set 2 prep?:\n",
    "        for_zcheck_S2R1=apply_th(th_S1R2,roi) \n",
    "        fatseg1_mask_final_S1,fatseg1_final_S1,fatseg2_mask_final_S1,fatseg2_final_S1,=get_final_fatsegs_S1(for_zcheck_S2R1*roi,th_S1R1,for_zcheck_S2R1,roi) \n",
    "\n",
    "        #Set 2: Round 1:\n",
    "        th_S2R1,fatseg_mask_S2R1,fatseg_S2R1=apply_ITSA_w_Z(roi,initial_th_S1R1,for_zcheck_S2R1,8) \n",
    "        \n",
    "        #Set 2: Round 2:\n",
    "        roi_for_S2R2=subtract_fat1(fatseg_S2R1) \n",
    "        initial_th_S2R2=initial_th(roi_for_S2R2)\n",
    "        th_S2R2,fatseg_mask_S2R2,fatseg_S2R2=apply_ITSA_w_Z(roi_for_S2R2,initial_th_S2R2,for_zcheck_S2R1,8) \n",
    "        \n",
    "        #Final connectivity check:\n",
    "        for_final_zcheck=apply_th(th_S2R2,roi) \n",
    "        final_fat_mask, final_fat=apply_final_zcheck(th_S2R2,for_final_zcheck,8) \n",
    "        fatseg_mask_final_R1,fatseg_final_R1,fatseg_mask_final_R2,fatseg_final_R2=get_final_fatsegs(final_fat,th_S2R1,final_fat_mask,roi)\n",
    "\n",
    "        #from set1:\n",
    "\n",
    "        for_zcheck_S2R1_L.append([])\n",
    "        fatseg1_mask_final_S1_L.append([])\n",
    "        fatseg1_final_S1_L.append([])\n",
    "        fatseg2_mask_final_S1_L.append([])\n",
    "        fatseg2_final_S1_L.append([])\n",
    "\n",
    "        for_zcheck_S2R1_L[ID_count]=for_zcheck_S2R1\n",
    "        fatseg1_mask_final_S1_L[ID_count]=fatseg1_mask_final_S1\n",
    "        fatseg1_final_S1_L[ID_count]=fatseg1_final_S1\n",
    "        fatseg2_mask_final_S1_L[ID_count]=fatseg2_mask_final_S1\n",
    "        fatseg2_final_S1_L[ID_count]=fatseg2_final_S1\n",
    "\n",
    "        #from set 2:\n",
    "        final_fat_mask_L.append([])\n",
    "        final_fat_L.append([])\n",
    "\n",
    "        fatseg_mask_final_R1_L.append([])\n",
    "        fatseg_final_R1_L.append([])\n",
    "        fatseg_mask_final_R2_L.append([])\n",
    "        fatseg_final_R2_L.append([])\n",
    "\n",
    "        final_fat_mask_L[ID_count]=final_fat_mask\n",
    "        final_fat_L[ID_count]=final_fat\n",
    "\n",
    "        fatseg_mask_final_R1_L[ID_count]=fatseg_mask_final_R1\n",
    "        fatseg_final_R1_L[ID_count]=fatseg_final_R1\n",
    "\n",
    "        fatseg_mask_final_R2_L[ID_count]=fatseg_mask_final_R2\n",
    "        fatseg_final_R2_L[ID_count]=fatseg_final_R2\n",
    "\n",
    "        print(f\"{ID_count}. {files} success\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba773db",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_spacing = image_OG.GetSpacing() #get spacing for original image - for calculations later\n",
    "ID=files\n",
    "results_df=[]\n",
    "for ID_count,files in enumerate(os.listdir(patpath)):\n",
    "        try:\n",
    "            ID=files\n",
    "            results_df.append([])    \n",
    "\n",
    "            final_F1_mask=fatseg_mask_final_R1_L[ID_count] \n",
    "            final_F1=fatseg_final_R1_L[ID_count]\n",
    "\n",
    "            final_F2_mask=fatseg_mask_final_R2_L[ID_count]\n",
    "            final_F2=fatseg_final_R2_L[ID_count]\n",
    "\n",
    "            final_BF_mask=final_fat_mask_L[ID_count]\n",
    "            final_BF=final_fat_L[ID_count]\n",
    "\n",
    "            final_roi_mask=roi_mask_L[ID_count]\n",
    "            final_roi=roi_mask_L[ID_count]*Ivol8c_roi_L[ID_count] \n",
    "\n",
    "\n",
    "            results_df[ID_count]=calc2(final_F1_mask,final_F1,final_F2_mask,final_F2,final_BF_mask,final_BF,final_roi_mask,final_roi)\n",
    "        except:\n",
    "            print(f\"{files} failed\")\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fdd5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "store_dir=r\"thighimage_IMFoutputs.xlsx\" #***change to excel file name\n",
    "\n",
    "for ID_count,files in enumerate(os.listdir(path)): \n",
    "        try:\n",
    "            if ID_count==0:#if first ID\n",
    "\n",
    "                with pd.ExcelWriter(store_dir) as writer:  \n",
    "                    results_df[ID_count].to_excel(writer, sheet_name='Results',index=False)    \n",
    "\n",
    "            else: #append to same SHEET (not just excel)     \n",
    "                results_to_append=append_df_to_sheet(results_df[ID_count],store_dir,'Results') #read sheet name \"Results\" and append new results_table\n",
    "                with pd.ExcelWriter(store_dir) as writer:  \n",
    "                    results_to_append.to_excel(writer, sheet_name='Results',index=False) \n",
    "\n",
    "        except:\n",
    "            print(f\"{files} export failed\")\n",
    "            # Load the existing Excel file into a DataFrame\n",
    "            df_excel = pd.read_excel(store_dir, sheet_name='Results')\n",
    "            # Create a new DataFrame with the data to append\n",
    "            data_to_append = pd.DataFrame({'Filename': [files], 'Error': ['Patient Mask Not Complete']})\n",
    "            # Concatenate the two DataFrames vertically\n",
    "            result = pd.concat([df_excel, data_to_append], ignore_index=True)\n",
    "            # Save the concatenated DataFrame back to the Excel file without index values\n",
    "            with pd.ExcelWriter(store_dir, engine='openpyxl') as writer:\n",
    "                result.to_excel(writer, sheet_name='Results', index=False)\n",
    "        else:\n",
    "            print(f\"{files} export success\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
